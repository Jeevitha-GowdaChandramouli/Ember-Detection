{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Dataset\n",
    "TRAIN_DATASET_ROOT = \"data/0408_take2_new/train/\"\n",
    "TEST_DATASET_ROOT = \"data/0408_take2_new/test/\"\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.0025\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "STEP_SIZE = 3\n",
    "GAMMA = 1\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert(boxes):\n",
    "    '''\n",
    "    Functionality: Convert bounding boxes format from xywh to xyxy\n",
    "    Input: boxes of the format xmin, ymin, width, height\n",
    "    Output: boxes of format xmin, ymin, xmax, ymax (top left and bottom right points)\n",
    "    '''\n",
    "    x, y, w, h = boxes.unbind(-1)\n",
    "    boxes = torch.stack([x, y, x + w, y + h], dim=-1)\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmberDataset_new(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        '''\n",
    "        Input: Dataset (Either the train or test data)\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"imgs\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Function : Fetches the image information along with the target given an index\n",
    "        Input : Index of the image\n",
    "        Output : Returns the image and target for the given index\n",
    "        '''\n",
    "        # load images\n",
    "        try:\n",
    "            img_path = os.path.join(self.root, \"imgs\", self.imgs[idx])\n",
    "            img = Image.open(img_path).convert(\"RGB\") \n",
    "            img = img.resize((224,224))\n",
    "            boxes = []\n",
    "        except:\n",
    "            print(f\"Idx number for which it failed: {idx}\")\n",
    "\n",
    "        # load annotations\n",
    "        annotation_path = os.path.join(self.root, \"annotations/\")\n",
    "        with open(annotation_path + os.listdir(annotation_path)[0], \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        temp = data[\"annotations\"][idx][\"bbox\"]\n",
    "        temp = torch.as_tensor(temp, dtype=torch.float32)\n",
    "        temp = temp.unsqueeze(0)\n",
    "        boxes = box_convert(temp)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area \n",
    "        target[\"image_id\"] = torch.tensor([data[\"annotations\"][idx][\"image_id\"]])\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    '''\n",
    "    Function : Transform the image into a tensor followed by flipping the images if train argument is true\n",
    "    Input : Train argument\n",
    "    Output : Transformed image to a tensor, flipped image if train argument is true\n",
    "    '''\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "    # during training, randomly flip the training images and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    '''\n",
    "    Function : To visualise the bounding box of an image\n",
    "    Input : Image to be visualised\n",
    "    Output : Ember image along with the bounding box\n",
    "    '''\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "boxes = torch.tensor([[1948.,  554., 2034.,  640.]], dtype=torch.float)\n",
    "\n",
    "colors = [\"yellow\"]\n",
    "ember_img = read_image(os.path.join(TRAIN_DATASET_ROOT,\"imgs\",'156.jpg'))\n",
    "result = draw_bounding_boxes(ember_img, boxes, colors=colors, width=5)\n",
    "show(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3 - Having two streams for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "res50_con = nn.Sequential(*list(model.children())[:-4])\n",
    "\n",
    "\n",
    "class upstream(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Function : \n",
    "        Initialise backbone to resnet models with certain layers detached\n",
    "        Initialise the Upsample model \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.resnet_modified_backbone = res50_con\n",
    "        self.upsample = nn.Upsample(scale_factor=(4,4), mode='nearest')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Function : Return the upstreamed data after passing through the Upsample function\n",
    "        Input : Image to be upstreamed\n",
    "        Output : Upstreamed image after passing through the transpose2D\n",
    "        '''\n",
    "        x = self.resnet_modified_backbone(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "class downstream(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialise conv layer and relu layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 256, 1)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Function : Return the downstreamed data after passing through the 1X1 convolution followed by Maxpool and ReLU\n",
    "        Input : Image to be downstreamed\n",
    "        Output : Downstreamed image after passing through the 1X1 convolution, maxpool and ReLU\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, upstream, downstream, num_classes):\n",
    "        '''\n",
    "        Initialise the upstreams, downstreams, relu activation and output channels\n",
    "        '''\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.upstream = upstream\n",
    "        self.downstream = downstream\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = 768\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function : To return the concatenation of the image from the upstream as well as downstream\n",
    "        Input : Image to be upstreamed and downstreamed\n",
    "        Output : Concatenated upstream and downstream image\n",
    "        '''\n",
    "        x1 = self.upstream(x)\n",
    "        x2 = self.downstream(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.relu(x)\n",
    "    \n",
    "\n",
    "background_supression_stream = upstream()\n",
    "small_object_stream = downstream()\n",
    "backbone = Ensemble(background_supression_stream, small_object_stream, num_classes = 2)\n",
    "model = FasterRCNN(backbone,num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER\n",
    "def data_loader_func():\n",
    "    '''\n",
    "    Function : Load the data from the train and test dataset roots\n",
    "    Output : Return the data loaders\n",
    "    '''\n",
    "   # use our dataset and defined transformations\n",
    "    dataset = EmberDataset_new(TRAIN_DATASET_ROOT, get_transform(train=True))\n",
    "    dataset_test = EmberDataset_new(TEST_DATASET_ROOT, get_transform(train=False))\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "\n",
    "\n",
    "# OPTIMIZER\n",
    "def optimizer(model):\n",
    "    '''\n",
    "    Function : Optimize the parameters by choosing the appropriate hyperparameters\n",
    "    Input : Model is taken as the input\n",
    "    Output : Model with the optimal parameters\n",
    "    '''\n",
    "    model.to(DEVICE)\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr = LEARNING_RATE,\n",
    "                                momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "    # and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size = STEP_SIZE,\n",
    "                                                gamma=GAMMA)\n",
    "    return lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "def training_function(model):\n",
    "    '''\n",
    "    Function : Train for the number of epochs after loading the data and optimising the model\n",
    "    Input : Model is given as the input\n",
    "    Output : Evaluated result\n",
    "    '''\n",
    "    data_loader, data_loader_test = data_loader_func()\n",
    "    lr_scheduler = optimizer(model)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, DEVICE, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=DEVICE)\n",
    "\n",
    "training_function(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5336e623b002d26a1f558f113d43641ae3b5f9eedd6e0d06bd5a4d9e3d81c743"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
